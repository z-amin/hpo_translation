{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2e9d5df770a7d1",
   "metadata": {},
   "source": [
    "# Running pre-trained models on biomedical texts to evaluate them\n",
    "## Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "id": "92381a6bc32b6463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.290010Z",
     "start_time": "2024-04-29T14:24:29.287184Z"
    }
   },
   "source": [
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install sacremoses\n",
    "# !pip install accelerate\n",
    "# !pip install ipywidgets\n",
    "# !pip install protobuf\n",
    "# !pip install pytorch"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c37f8cd1cb3e5a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.300272Z",
     "start_time": "2024-04-29T14:24:29.296582Z"
    }
   },
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class TranslationModel:\n",
    "    def __init__(self, checkpoint_name: str):\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def translate(self, source: str) -> str:\n",
    "        \"\"\"Translates a source text with the model\n",
    "        :param source: the text to translate\n",
    "        :return: str - the translation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.checkpoint_name"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f169902b3db78872",
   "metadata": {},
   "source": [
    "### Helsinki-NLP/opus-mt-en-es"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ec64c835e29d26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.314803Z",
     "start_time": "2024-04-29T14:24:29.310562Z"
    }
   },
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "class HelsinkiNLPModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        self.model = MarianMTModel.from_pretrained(self.checkpoint_name)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer.encode(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(input_ids, num_beams=4, early_stopping=True)\n",
    "        translated_text = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        return translated_text"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "20496f80d7c7e5ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595ea80d96deb59b",
   "metadata": {},
   "source": [
    "### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc78bc194ead970d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.321498Z",
     "start_time": "2024-04-29T14:24:29.316254Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "class NLLBModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name, device_map=\"auto\")\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        inputs = self.tokenizer(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(\n",
    "            **inputs, forced_bos_token_id=self.tokenizer.lang_code_to_id[\"spa_Latn\"], max_length=30\n",
    "        )\n",
    "        translated_text = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated_text"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "412b720dfdb86448",
   "metadata": {},
   "source": "## Madlad (Google T5)"
  },
  {
   "cell_type": "code",
   "id": "ee607bde81fbab72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.329499Z",
     "start_time": "2024-04-29T14:24:29.323830Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "class MadladModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__('jbochi/madlad400-3b-mt')\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.checkpoint_name).to(device)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer(f\"<2es> {source}\", max_length=512, truncation=True,\n",
    "                                   return_tensors=\"pt\").input_ids.to(device)\n",
    "        outputs = self.model.generate(input_ids=input_ids, max_new_tokens=512, num_beams=4, early_stopping=True)\n",
    "        translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translated_text\n"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1c17fb211e964c4",
   "metadata": {},
   "source": [
    "## Loading and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b2b36bc1be316f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:24:29.339422Z",
     "start_time": "2024-04-29T14:24:29.331Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pretrained_models.preliminary_experiments.translation_model import TranslationModel, NLLBModel\n",
    "\n",
    "\n",
    "def load_sentences(test_dataset: str, num_rows=1000, seed=42) -> pd.DataFrame:\n",
    "    \"\"\"Loads a test dataset in .jsonl format into a dataframe and randomly selects n rows\n",
    "    :param test_dataset: filename of the test dataset\n",
    "    :param num_rows: number of rows to select\n",
    "    :param seed: random seed for reproducibility\n",
    "    :return pd.Dataframe: the parallel corpus as a dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(test_dataset, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.sample(n=num_rows, random_state=seed)\n",
    "\n",
    "\n",
    "def run_models(model: TranslationModel, test_sentences: pd.DataFrame, dataset_name: str):\n",
    "    \"\"\"Runs a model on the test sentences. Creates a dataframe for the results with two columns: 'Reference' and 'Actual'\n",
    "    :param model: the MT model to evaluate\n",
    "    :param test_sentences: a dataframe for a parallel corpus\"\"\"\n",
    "\n",
    "    dataset_without_extension = dataset_name.split(\".jsonl\")[0]\n",
    "    folder = f\"translations/{dataset_without_extension}/\"\n",
    "\n",
    "    print(f\"\\tModel: {model}\")\n",
    "    df = pd.DataFrame(columns=['reference', 'actual'])\n",
    "\n",
    "    for _, row in tqdm(test_sentences.iterrows()):\n",
    "        english, spanish = row['en'], row['es']\n",
    "        translation = model.translate(english)\n",
    "        df.loc[len(df.index)] = [spanish, translation]\n",
    "\n",
    "    result_filename = folder + model.checkpoint_name.split(\"/\")[0] + \".csv\"\n",
    "    os.makedirs(os.path.dirname(result_filename), exist_ok=True)\n",
    "    df.to_csv(result_filename)\n",
    "\n",
    "\n",
    "def evaluate_on_all_test_data(translation_model: TranslationModel, dataset_names: list[str],\n",
    "                              corpus_directory: str):\n",
    "    for test_dataset in dataset_names:\n",
    "        test_path = corpus_directory + test_dataset\n",
    "        print(f\"Test dataset: {test_dataset}\")\n",
    "        test_sentences = load_sentences(test_path, num_rows=3)\n",
    "        run_models(translation_model, test_sentences, test_dataset)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cef891a86bb7091d",
   "metadata": {},
   "source": [
    "## Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "id": "92159dcf4eaca7d8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-29T14:24:29.340696Z"
    }
   },
   "source": [
    "test_directory = \"/home/zakiamin/PycharmProjects/hpo_translation/corpus/train/\"\n",
    "# \"/Users/zaki/PycharmProjects/hpo_translation/corpus/train/\"\n",
    "filenames = [\"abstracts.jsonl\", \"khresmoi.jsonl\", \"medline.jsonl\", \"scielo.jsonl\", \"snomed.jsonl\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "72ab51328f9f73e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "evaluate_on_all_test_data(HelsinkiNLPModel(), filenames, test_directory)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "evaluate_on_all_test_data(NLLBModel(), filenames, test_directory)",
   "id": "cef145ce1b540472",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "evaluate_on_all_test_data(MadladModel(), filenames, test_directory)",
   "id": "8c0f258292f910c0",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
