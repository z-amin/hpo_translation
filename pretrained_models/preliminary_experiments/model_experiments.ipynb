{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running pre-trained models on biomedical texts to evaluate them\n",
    "## Infrastructure"
   ],
   "id": "6c2e9d5df770a7d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:18:43.026671Z",
     "start_time": "2024-04-15T16:18:43.009084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "using_gpu = False\n",
    "device = \"gpu\" if using_gpu else \"cpu\"\n",
    "\n",
    "\n",
    "class TranslationModel:\n",
    "    def __init__(self, checkpoint_name: str):\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def translate(self, source: str) -> str:\n",
    "        \"\"\"Translates a source text with the model\n",
    "        :param source: the text to translate\n",
    "        :return: str - the translation\"\"\"\n",
    "        pass"
   ],
   "id": "c37f8cd1cb3e5a47",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:19:23.058785Z",
     "start_time": "2024-04-15T16:18:43.035405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "!pip install nltk\n",
    "!pip install sentence_transformers\n",
    "!pip install sacrebleu\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import sacrebleu\n",
    "\n",
    "similarity_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "class SimilarityMetric(Enum):\n",
    "    \"\"\"Enum for string similarity metrics. Each metric must implement the evaluate method.\"\"\"\n",
    "    BLEU = 0\n",
    "    SIMPLE = 1\n",
    "    EDIT_DISTANCE = 2\n",
    "    SEMANTIC_SIMILARITY = 3\n",
    "    SACREBLEU = 4\n",
    "\n",
    "    def evaluate(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Evaluate the given string similarity metric between two strings.\n",
    "        Performs simple string cleaning for whitespace and punctuation.\n",
    "        :param reference: reference and official term\n",
    "        :param candidate: model-produced translated term\n",
    "        :return: similarity score when evaluating this specific metric\n",
    "        \"\"\"\n",
    "        if self == SimilarityMetric.BLEU:\n",
    "            reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "            candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "            return sentence_bleu([reference_tokens], candidate_tokens,\n",
    "                                 smoothing_function=SmoothingFunction().method1)\n",
    "        elif self == SimilarityMetric.SIMPLE:\n",
    "            return 1 if reference == candidate else 0\n",
    "        elif self == SimilarityMetric.EDIT_DISTANCE:\n",
    "            return 1 - nltk.edit_distance(reference, candidate) / max(len(reference), len(candidate))\n",
    "        elif self == SimilarityMetric.SEMANTIC_SIMILARITY:\n",
    "            query_embedding = similarity_model.encode(reference)\n",
    "            passage_embedding = similarity_model.encode(candidate)\n",
    "            cosine_similarity = util.cos_sim(query_embedding, passage_embedding)\n",
    "            return cosine_similarity[0].item()\n",
    "        else:\n",
    "            bleu = sacrebleu.raw_corpus_bleu(candidate, [reference])\n",
    "            return bleu.score"
   ],
   "id": "df4d7bb71918990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: nltk in /usr/local/lib/python3.9/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2023.12.25)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk) (4.66.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.9/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (4.66.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (2.1.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (1.26.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (1.1.3)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (1.11.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (0.22.2)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/site-packages (from sentence_transformers) (10.0.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.12.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.9.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: sacrebleu in /usr/local/lib/python3.9/site-packages (2.4.2)\r\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/site-packages (from sacrebleu) (2.8.2)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/site-packages (from sacrebleu) (2023.12.25)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from sacrebleu) (1.26.0)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.9/site-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/site-packages (from sacrebleu) (5.2.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helsinki-NLP/opus-mt-en-es",
   "id": "f169902b3db78872"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:19:35.590511Z",
     "start_time": "2024-04-15T16:19:23.062482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import sacremoses\n",
    "\n",
    "\n",
    "class HelsinkiNLPModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        self.model = MarianMTModel.from_pretrained(self.checkpoint_name)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer.encode(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(input_ids, num_beams=4, early_stopping=True)\n",
    "        translated_text = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        return translated_text"
   ],
   "id": "3ec64c835e29d26f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.39.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.12.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.9/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/site-packages (from transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.66.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/site-packages (0.2.0)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "20496f80d7c7e5ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### facebook/nllb-200-distilled-600M",
   "id": "595ea80d96deb59b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:19:35.602847Z",
     "start_time": "2024-04-15T16:19:35.593617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "class NLLBModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        inputs = self.tokenizer(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(\n",
    "            **inputs, forced_bos_token_id=self.tokenizer.lang_code_to_id[\"spa_Latn\"], max_length=30\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ],
   "id": "dc78bc194ead970d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### google-t5",
   "id": "412b720dfdb86448"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:19:35.634486Z",
     "start_time": "2024-04-15T16:19:35.608388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "class T5Model(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"google-t5/t5-small\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_text = \"translate English to Spanish: \" + source\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512)\n",
    "        outputs = self.model.generate(input_ids=input_ids, num_beams=4, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ],
   "id": "ee607bde81fbab72",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading test data",
   "id": "1c17fb211e964c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:19:36.718934Z",
     "start_time": "2024-04-15T16:19:35.637550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pretrained_models.preliminary_experiments.translation_model import TranslationModel, NLLBModel\n",
    "\n",
    "\n",
    "def load_sentences(test_dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a test dataset in .jsonl format into a dataframe\n",
    "    :param test_dataset: filename of the test dataset\n",
    "    :return pd.Dataframe: the parallel corpus as a dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(test_dataset, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def evaluate_models_on(mt_models: list[TranslationModel], test_sentences: pd.DataFrame) -> dict[\n",
    "    TranslationModel, dict[SimilarityMetric, float]]:\n",
    "    \"\"\"For each model, obtains the average for all metrics over all test sentences.\n",
    "    :param mt_models: the list of translation models to evaluate\n",
    "    :param test_sentences: a dataframe for a parallel corpus\"\"\"\n",
    "\n",
    "    results = {model: {metric: 0 for metric in SimilarityMetric} for model in mt_models}\n",
    "\n",
    "    for model in mt_models:\n",
    "        print(f\"Model: {model}\")\n",
    "        for _, row in tqdm(test_sentences.iterrows()):\n",
    "            english, spanish = row['en'], row['es']\n",
    "            for metric in SimilarityMetric:\n",
    "                similarity = metric.evaluate(spanish, model.translate(english))\n",
    "                results[model][metric] += similarity\n",
    "\n",
    "    n = test_sentences.shape[0]\n",
    "    for model in mt_models:\n",
    "        for metric in results[model]:\n",
    "            results[model][metric] /= n\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_on_all_test_data(translation_models: list[TranslationModel], test_datasets: list[str]):\n",
    "    for test_dataset in test_datasets:\n",
    "        print(f\"Test dataset: {test_dataset}\")\n",
    "        test_sentences = load_sentences(test_dataset)\n",
    "        model_metrics = evaluate_models_on(translation_models, test_sentences)\n",
    "        print(model_metrics)\n",
    "\n",
    "\n",
    "directory_prefix = \"/Users/zaki/PycharmProjects/hpo_translation/corpus/test/\"\n",
    "filenames = [\"abstract5.jsonl\"]  # + [\"abstracts.jsonl\", \"clinspen.jsonl\", \"khresmoi.jsonl\"]\n",
    "all_test_datasets = [directory_prefix + filename for filename in filenames]"
   ],
   "id": "8b2b36bc1be316f1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating the models",
   "id": "cef891a86bb7091d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:23:55.782361Z",
     "start_time": "2024-04-15T16:19:36.721739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_models = [HelsinkiNLPModel(), NLLBModel(), T5Model()]\n",
    "evaluate_on_all_test_data(all_models, all_test_datasets)"
   ],
   "id": "72ab51328f9f73e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: /Users/zaki/PycharmProjects/hpo_translation/corpus/test/abstract5.jsonl\n",
      "Model: <__main__.HelsinkiNLPModel object at 0x1302f0af0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:03, 12.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <pretrained_models.preliminary_experiments.translation_model.NLLBModel object at 0x1319298e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]the `lang_code_to_id` attribute is deprecated. The logic is natively handled in the `tokenizer.adder_tokens_decoder` this attribute will be removed in `transformers` v4.38\n",
      "5it [01:28, 17.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <__main__.T5Model object at 0x1318ece50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.9/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "5it [00:16,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<__main__.HelsinkiNLPModel object at 0x1302f0af0>: {<SimilarityMetric.BLEU: 0>: 0.31283084407223, <SimilarityMetric.SIMPLE: 1>: 0.0, <SimilarityMetric.EDIT_DISTANCE: 2>: 0.6284120710816267, <SimilarityMetric.SEMANTIC_SIMILARITY: 3>: 0.951473867893219, <SimilarityMetric.SACREBLEU: 4>: 18.920882472626918}, <pretrained_models.preliminary_experiments.translation_model.NLLBModel object at 0x1319298e0>: {<SimilarityMetric.BLEU: 0>: 0.1778587805216802, <SimilarityMetric.SIMPLE: 1>: 0.0, <SimilarityMetric.EDIT_DISTANCE: 2>: 0.4411531474040361, <SimilarityMetric.SEMANTIC_SIMILARITY: 3>: 0.8756338953971863, <SimilarityMetric.SACREBLEU: 4>: 24.861797534566787}, <__main__.T5Model object at 0x1318ece50>: {<SimilarityMetric.BLEU: 0>: 0.003911432380992995, <SimilarityMetric.SIMPLE: 1>: 0.0, <SimilarityMetric.EDIT_DISTANCE: 2>: 0.17933786188169604, <SimilarityMetric.SEMANTIC_SIMILARITY: 3>: 0.7251034379005432, <SimilarityMetric.SACREBLEU: 4>: 3.4863833608273813}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
