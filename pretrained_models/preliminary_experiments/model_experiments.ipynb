{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running pre-trained models on biomedical texts to evaluate them\n",
    "## Infrastructure"
   ],
   "id": "6c2e9d5df770a7d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:02:31.572566Z",
     "start_time": "2024-04-16T09:02:31.563457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "using_gpu = False\n",
    "device = \"gpu\" if using_gpu else \"cpu\"\n",
    "\n",
    "\n",
    "class TranslationModel:\n",
    "    def __init__(self, checkpoint_name: str):\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def translate(self, source: str) -> str:\n",
    "        \"\"\"Translates a source text with the model\n",
    "        :param source: the text to translate\n",
    "        :return: str - the translation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.checkpoint_name"
   ],
   "id": "c37f8cd1cb3e5a47",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-16T09:02:31.600645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "!pip install nltk\n",
    "!pip install sentence_transformers\n",
    "!pip install sacrebleu\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import sacrebleu\n",
    "\n",
    "similarity_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "class SimilarityMetric(Enum):\n",
    "    \"\"\"Enum for string similarity metrics. Each metric must implement the evaluate method.\"\"\"\n",
    "    BLEU = 0\n",
    "    SIMPLE = 1\n",
    "    EDIT_DISTANCE = 2\n",
    "    SEMANTIC_SIMILARITY = 3\n",
    "    SACREBLEU = 4\n",
    "\n",
    "    def evaluate(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Evaluate the given string similarity metric between two strings.\n",
    "        Performs simple string cleaning for whitespace and punctuation.\n",
    "        :param reference: reference and official term\n",
    "        :param candidate: model-produced translated term\n",
    "        :return: similarity score when evaluating this specific metric\n",
    "        \"\"\"\n",
    "        if self == SimilarityMetric.BLEU:\n",
    "            reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "            candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "            return sentence_bleu([reference_tokens], candidate_tokens,\n",
    "                                 smoothing_function=SmoothingFunction().method1)\n",
    "        elif self == SimilarityMetric.SIMPLE:\n",
    "            return 1 if reference == candidate else 0\n",
    "        elif self == SimilarityMetric.EDIT_DISTANCE:\n",
    "            return 1 - nltk.edit_distance(reference, candidate) / max(len(reference), len(candidate))\n",
    "        elif self == SimilarityMetric.SEMANTIC_SIMILARITY:\n",
    "            query_embedding = similarity_model.encode(reference)\n",
    "            passage_embedding = similarity_model.encode(candidate)\n",
    "            cosine_similarity = util.cos_sim(query_embedding, passage_embedding)\n",
    "            return cosine_similarity[0].item()\n",
    "        else:\n",
    "            bleu = sacrebleu.raw_corpus_bleu(candidate, [reference])\n",
    "            return bleu.score"
   ],
   "id": "df4d7bb71918990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: nltk in /usr/local/lib/python3.9/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2023.12.25)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk) (4.66.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helsinki-NLP/opus-mt-en-es",
   "id": "f169902b3db78872"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "class HelsinkiNLPModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        self.model = MarianMTModel.from_pretrained(self.checkpoint_name)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer.encode(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(input_ids, num_beams=4, early_stopping=True)\n",
    "        translated_text = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        return translated_text"
   ],
   "id": "3ec64c835e29d26f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "20496f80d7c7e5ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### facebook/nllb-200-distilled-600M",
   "id": "595ea80d96deb59b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "class NLLBModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        inputs = self.tokenizer(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(\n",
    "            **inputs, forced_bos_token_id=self.tokenizer.lang_code_to_id[\"spa_Latn\"], max_length=30\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ],
   "id": "dc78bc194ead970d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### google-t5",
   "id": "412b720dfdb86448"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "class T5Model(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"google-t5/t5-small\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_text = \"translate English to Spanish: \" + source\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512)\n",
    "        outputs = self.model.generate(input_ids=input_ids, num_beams=4, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ],
   "id": "ee607bde81fbab72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and evaluation functions",
   "id": "1c17fb211e964c4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "\n",
    "from pretrained_models.preliminary_experiments.translation_model import TranslationModel, NLLBModel\n",
    "\n",
    "\n",
    "def load_sentences(test_dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a test dataset in .jsonl format into a dataframe\n",
    "    :param test_dataset: filename of the test dataset\n",
    "    :return pd.Dataframe: the parallel corpus as a dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(test_dataset, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def evaluate_models_on(mt_models: list[TranslationModel], test_sentences: pd.DataFrame) -> dict[\n",
    "    TranslationModel, dict[SimilarityMetric, float]]:\n",
    "    \"\"\"For each model, obtains the average for all metrics over all test sentences.\n",
    "    :param mt_models: the list of translation models to evaluate\n",
    "    :param test_sentences: a dataframe for a parallel corpus\"\"\"\n",
    "\n",
    "    results = {model: {metric: 0 for metric in SimilarityMetric} for model in mt_models}\n",
    "\n",
    "    for model in mt_models:\n",
    "        print(f\"Model: {model}\")\n",
    "        for _, row in tqdm(test_sentences.iterrows()):\n",
    "            english, spanish = row['en'], row['es']\n",
    "            for metric in SimilarityMetric:\n",
    "                similarity = metric.evaluate(spanish, model.translate(english))\n",
    "                results[model][metric] += similarity\n",
    "\n",
    "    n = test_sentences.shape[0]\n",
    "    for model in mt_models:\n",
    "        for metric in results[model]:\n",
    "            results[model][metric] /= n\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_on_all_test_data(translation_models: list[TranslationModel], test_datasets: list[str]):\n",
    "    for test_dataset in test_datasets:\n",
    "        print(f\"Test dataset: {test_dataset}\")\n",
    "        test_sentences = load_sentences(test_dataset)\n",
    "        model_metrics = evaluate_models_on(translation_models, test_sentences)\n",
    "        pprint.pprint(model_metrics)\n",
    "\n",
    "\n",
    "directory_prefix = \"/Users/zaki/PycharmProjects/hpo_translation/corpus/test/\"\n",
    "filenames = [\"abstract5.jsonl\"]  # + [\"abstracts.jsonl\", \"clinspen.jsonl\", \"khresmoi.jsonl\"]\n",
    "all_test_datasets = [directory_prefix + filename for filename in filenames]"
   ],
   "id": "8b2b36bc1be316f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating the models",
   "id": "cef891a86bb7091d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "all_models = [HelsinkiNLPModel(), NLLBModel(), T5Model()]\n",
    "evaluate_on_all_test_data(all_models, all_test_datasets)"
   ],
   "id": "72ab51328f9f73e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
