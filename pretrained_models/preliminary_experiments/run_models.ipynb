{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2e9d5df770a7d1",
   "metadata": {},
   "source": [
    "# Running pre-trained models on biomedical texts to evaluate them\n",
    "## Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "id": "92381a6bc32b6463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:57:36.584096Z",
     "start_time": "2024-04-29T11:56:59.325382Z"
    }
   },
   "source": [
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install accelerate\n",
    "!pip install ipywidgets\n",
    "!pip install protobuf"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.39.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.12.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.9/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/site-packages (from transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.66.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/site-packages (0.2.0)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: sacremoses in /usr/local/lib/python3.9/site-packages (0.1.1)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/site-packages (from sacremoses) (2023.12.25)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from sacremoses) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from sacremoses) (1.3.2)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from sacremoses) (4.66.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.9/site-packages (0.29.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from accelerate) (1.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from accelerate) (23.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from accelerate) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/site-packages (from accelerate) (2.1.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/site-packages (from accelerate) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/site-packages (from accelerate) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: ipywidgets in /usr/local/lib/python3.9/site-packages (8.1.2)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (0.1.4)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (8.16.1)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (5.10.1)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (4.0.10)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (3.0.10)\r\n",
      "Requirement already satisfied: backcall in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\r\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\r\n",
      "Requirement already satisfied: appnope in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\r\n",
      "Requirement already satisfied: pure-eval in /Users/zaki/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: protobuf in /usr/local/lib/python3.9/site-packages (5.26.1)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "c37f8cd1cb3e5a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:57:36.605433Z",
     "start_time": "2024-04-29T11:57:36.591781Z"
    }
   },
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "using_gpu = False\n",
    "device = \"gpu\" if using_gpu else \"cpu\"\n",
    "\n",
    "\n",
    "class TranslationModel:\n",
    "    def __init__(self, checkpoint_name: str):\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def translate(self, source: str) -> str:\n",
    "        \"\"\"Translates a source text with the model\n",
    "        :param source: the text to translate\n",
    "        :return: str - the translation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.checkpoint_name"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "f169902b3db78872",
   "metadata": {},
   "source": [
    "### Helsinki-NLP/opus-mt-en-es"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ec64c835e29d26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:57:36.621504Z",
     "start_time": "2024-04-29T11:57:36.608592Z"
    }
   },
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "class HelsinkiNLPModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        self.model = MarianMTModel.from_pretrained(self.checkpoint_name)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(self.checkpoint_name)\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer.encode(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(input_ids, num_beams=4, early_stopping=True)\n",
    "        translated_text = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        return translated_text"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "20496f80d7c7e5ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595ea80d96deb59b",
   "metadata": {},
   "source": [
    "### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc78bc194ead970d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:57:36.647037Z",
     "start_time": "2024-04-29T11:57:36.629334Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "class NLLBModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name, device_map=\"auto\")\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        inputs = self.tokenizer(source, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(\n",
    "            **inputs, forced_bos_token_id=self.tokenizer.lang_code_to_id[\"spa_Latn\"], max_length=30\n",
    "        )\n",
    "        translated_text = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated_text"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "412b720dfdb86448",
   "metadata": {},
   "source": "## Madlad (Google T5)"
  },
  {
   "cell_type": "code",
   "id": "ee607bde81fbab72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T12:00:52.947146Z",
     "start_time": "2024-04-29T12:00:52.917567Z"
    }
   },
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "class MadladModel(TranslationModel):\n",
    "    def __init__(self):\n",
    "        super().__init__('jbochi/madlad400-3b-mt')\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.checkpoint_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.checkpoint_name, device_map=\"auto\")\n",
    "\n",
    "    def translate(self, source: str) -> str:\n",
    "        input_ids = self.tokenizer(f\"<2es> {source}\", return_tensors=\"pt\").input_ids.to(self.model.device)\n",
    "        outputs = self.model.generate(input_ids=input_ids)\n",
    "        translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translated_text\n"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "1c17fb211e964c4",
   "metadata": {},
   "source": [
    "## Loading and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b2b36bc1be316f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:57:36.693307Z",
     "start_time": "2024-04-29T11:57:36.674047Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pretrained_models.preliminary_experiments.translation_model import TranslationModel, NLLBModel\n",
    "\n",
    "\n",
    "def load_sentences(test_dataset: str, num_rows=1000, seed=42) -> pd.DataFrame:\n",
    "    \"\"\"Loads a test dataset in .jsonl format into a dataframe and randomly selects n rows\n",
    "    :param test_dataset: filename of the test dataset\n",
    "    :param num_rows: number of rows to select\n",
    "    :param seed: random seed for reproducibility\n",
    "    :return pd.Dataframe: the parallel corpus as a dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(test_dataset, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.sample(n=num_rows, random_state=seed)\n",
    "\n",
    "\n",
    "def run_models(mt_models: list[TranslationModel], test_sentences: pd.DataFrame, dataset_name: str):\n",
    "    \"\"\"Runs each model on the test sentences. Creates a dataframe for the results with two columns: 'Reference' and 'Actual'\n",
    "    :param mt_models: the list of translation models to evaluate\n",
    "    :param test_sentences: a dataframe for a parallel corpus\"\"\"\n",
    "\n",
    "    dataset_without_extension = dataset_name.split(\".jsonl\")[0]\n",
    "    folder = f\"translations/{dataset_without_extension}/\"\n",
    "\n",
    "    for model in mt_models:\n",
    "        print(f\"\\tModel: {model}\")\n",
    "        df = pd.DataFrame(columns=['reference', 'actual'])\n",
    "\n",
    "        for _, row in tqdm(test_sentences.iterrows()):\n",
    "            english, spanish = row['en'], row['es']\n",
    "            translation = model.translate(english)\n",
    "            df.loc[len(df.index)] = [spanish, translation]\n",
    "\n",
    "        result_filename = folder + model.checkpoint_name.split(\"/\")[0] + \".csv\"\n",
    "        os.makedirs(os.path.dirname(result_filename), exist_ok=True)\n",
    "        df.to_csv(result_filename)\n",
    "\n",
    "\n",
    "def evaluate_on_all_test_data(translation_models: list[TranslationModel], dataset_names: list[str],\n",
    "                              corpus_directory: str):\n",
    "    for test_dataset in dataset_names:\n",
    "        test_path = corpus_directory + test_dataset\n",
    "        print(f\"Test dataset: {test_dataset}\")\n",
    "        test_sentences = load_sentences(test_path, num_rows=5)\n",
    "\n",
    "        run_models(translation_models, test_sentences, test_dataset)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "cef891a86bb7091d",
   "metadata": {},
   "source": [
    "## Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "id": "92159dcf4eaca7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T12:01:33.796687Z",
     "start_time": "2024-04-29T12:00:57.318820Z"
    }
   },
   "source": [
    "all_models = [MadladModel()]\n",
    "# HelsinkiNLPModel(), NLLBModel(), "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "72ab51328f9f73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T12:05:01.199061Z",
     "start_time": "2024-04-29T12:02:41.816049Z"
    }
   },
   "source": [
    "test_directory = \"/Users/zaki/PycharmProjects/hpo_translation/corpus/train/\"\n",
    "filenames = [\"abstracts.jsonl\", \"khresmoi.jsonl\", \"medline.jsonl\", \"scielo.jsonl\", \"snomed.jsonl\"]\n",
    "\n",
    "evaluate_on_all_test_data(all_models, filenames, test_directory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: abstracts.jsonl\n",
      "\tModel: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:21,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: khresmoi.jsonl\n",
      "\tModel: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:26,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: medline.jsonl\n",
      "\tModel: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:57, 11.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: scielo.jsonl\n",
      "\tModel: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:25,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: snomed.jsonl\n",
      "\tModel: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:07,  1.43s/it]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87d37741f1c91558"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
